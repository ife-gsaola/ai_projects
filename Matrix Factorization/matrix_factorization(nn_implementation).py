# -*- coding: utf-8 -*-
"""Matrix Factorization(NN Implementation).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aSjNcNcOCl4bkqUU7_s6s-SQ37HjuFpa
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

import tensorflow as tf
import keras


sns.set()

data = pd.read_csv('shopping_trends_updated.csv')

"""##The dataset encompasses a wide array of features, each providing a unique perspective into the habits and preferences of customers."""

# Statistical description (mean, standard deviation and quantiles)

data.describe(include='all')

numerical_data = data.describe(include='all')

categorical_data = data.describe(include=['object'])
categorical_data

data.isnull().sum()

data['Gender'].value_counts()

data.head()

"""## DATA PREPROCESSING"""

# Sample item features (price, category, size, frequency_of_purchase)
item_features = data[['Category', 'Purchase Amount (USD)',	'Size', 'Subscription Status',	'Previous Purchases',	'Frequency of Purchases']]

item_purs_map = {'Blouse':0, 'Sweater':1, 'Jeans':2, 'Sandals':3, 'Sneakers':4, 'Shirt':5, 'Shorts':6, 'Coat':7,
                  'Handbag':8, 'Shoes':9, 'Dress':10, 'Skirt':11, 'Sunglasses':12, 'Pants':13, 'Jacket':14, 'Hoodie':15,
                  'Jewelry':16, 'T-shirt':17, 'Scarf':18, 'Hat':19, 'Socks':20, 'Backpack':21, 'Belt':22, 'Boots':23, 'Gloves':24}

cat_map = {'Clothing':0, 'Footwear':1, 'Outerwear':2, 'Accessories':3}

size_map = {'L':0, 'S':1, 'M':2, 'XL':3}

SubStat_map = {'Yes':0, 'No':1}

FreqPur_map = {'Fortnightly':0, 'Weekly':1, 'Annually':2, 'Quarterly':3, 'Bi-Weekly':4, 'Monthly':5, 'Every 3 Months':6}

item_features['Category'] = item_features['Category'].map(cat_map)
item_features['Size'] = item_features['Size'].map(size_map)
item_features['Subscription Status'] = item_features['Subscription Status'].map(SubStat_map)
item_features['Frequency of Purchases'] = item_features['Frequency of Purchases'].map(FreqPur_map)
item_features['itemid'] = data['Item Purchased'].map(item_purs_map)

# Adding customer ID and ratings to the item feature dataset
item_features['customerid'] = data['Customer ID']
item_features['ratings'] = data['Review Rating']

import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# selecting first 10 users for the recommendation system to save time and space complexity
item_features = item_features.iloc[:100,:]

# Preprocess the data
num_users = item_features['customerid'].nunique()
num_items = item_features['itemid'].nunique()
ratings = item_features['ratings'].values
user_ids = item_features['customerid'].values
item_ids = item_features['itemid'].values

# Convert user and item IDs to zero-based indices
user_indices = user_ids - 1
item_indices = item_ids - 1

# Additional features to be incorporated
additional_feat = item_features[['Category', 'Purchase Amount (USD)', 'Size', 'Subscription Status', 'Previous Purchases', 'Frequency of Purchases']].values
add_feat = item_features[['Category', 'Purchase Amount (USD)', 'Size', 'Subscription Status', 'Previous Purchases', 'Frequency of Purchases']]

# Standardize additional features
scaler = MinMaxScaler()
additional_features = scaler.fit_transform(additional_feat)

# Extracting the needed features into matrixfact_data
matrixfact_data = pd.DataFrame()

matrixfact_data['itemid'] = data['Item Purchased'].map(item_purs_map)

# Adding customer ID and ratings to the item feature dataset
matrixfact_data['customerid'] = data['Customer ID']
matrixfact_data['ratings'] = data['Review Rating']

matrixfact_data = matrixfact_data.iloc[:100,:]

# import numpy as np
# import pandas as pd
# from sklearn.preprocessing import StandardScaler, MinMaxScaler

# # selecting first 10 users for the recommendation system to save time and space complexity
# MFdata = matrixfact_data.iloc[:100,:]

additional_feat = item_features[['Category', 'Purchase Amount (USD)', 'Size', 'Subscription Status', 'Previous Purchases', 'Frequency of Purchases']].values

"""## CLUSTERING

**Implementation**
######K-Means clustering algorithm was used to analyze customer segmentation based on two
key features: ”Purchase Amount (USD)” and ”Previous Purchases.” Below outlines the
implementation steps:
##• Data Preprocessing: Relevant features that is, ”Purchase Amount (USD)” and
”Previous Purchases,” were selected from the dataset and standardized using the
StandardScaler from scikit-learn library to ensure each feature contributes equally
to the clustering process.
##• Determining Optimal Number of Clusters: Utilizing the Elbow Method,the
optimal number of clusters was determined by iterating K-Means with varying
cluster numbers from 1 to 10. The Within-Cluster Sum of Squares (WCSS) was
calculated for each cluster configuration, aiming to identify the ”elbow point,”
indicating the optimal number of clusters to be 4.
##• Model Training: the K-Means algorithm was instantiated with the determined
optimal number of clusters. Additionally, parameters such as the initialization
method (’k-means++’) and the random state for reproducibility were specified.
##• Cluster Assignment: The K-Means algorithm was trained on the scaled data,
and cluster labels were assigned to each data point.
"""

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

data_scaled = item_features[['Previous Purchases','Purchase Amount (USD)']]

data_scaled.head()

# Scale the data
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_scaled)

from sklearn.cluster import KMeans
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
    kmeans.fit(data_scaled)
    # inertia method returns wcss for that model
    wcss.append(kmeans.inertia_)

plt.figure(figsize=(7,4))
sns.lineplot(x=range(1, 11), y=wcss, marker='o', color='red')
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.grid(True)
plt.show()

kmeans = KMeans(n_clusters = 4, n_init='auto', random_state=5)
kmeans.fit(data_scaled)

labels1 = kmeans.labels_

labels1

plt.figure(figsize=(8, 4))

# Plot each pair of scaled features against each other
plt.scatter(x=data_scaled[:,1], y=data_scaled[:, 0], c=labels1, cmap='viridis', s=50)
plt.scatter(x=kmeans.cluster_centers_[:, 0], y=kmeans.cluster_centers_[:, 1], s=300, c='red', alpha=0.5)
plt.xlabel('Purchase Amount (USD)')
plt.ylabel('Previous Purchases')
plt.title('Clusters of Purchase Amount vs Previous Purchases')
plt.colorbar(label='Cluster Label')

plt.tight_layout()
plt.show()

"""From the scatterplot, ”Purchase Amount (USD)” on the
x-axis and ”Previous Purchases” on the y-axis. Each data point was colored ac-
cording to its assigned cluster label, facilitating the visual identification of different
customer segments. Additionally, we plotted the cluster centroids (representative
points) on the scatter plot to indicate the central tendencies of each cluster.
From the plot below, there are 4 clusters representing individuals with higher
previous purchase and lower purchase amount (purple), lower previous purchase
and lower purchase amount (yellow), lower previous purchase and higher purchase
amount (blue) and higher previous purchase and higher purchase amount (green)
.

## MATRIX FACTORIZATION

Traditional Matrix Factorization
In the traditional matrix factorization method implemented, the user-item interaction
matrix R was factorized into three matrices: P , Q, and F , representing user latent
factors, item latent factors, and additional item features, respectively.
The predicted rating ˆRui for user u and item i can be calculated as the dot product of
the corresponding latent feature vectors:
24
ˆRui =
K∑
k=1
Puk × Qik × Fjk
where:
• Puk represents the kth latent feature of user u.
• Qik represents the kth latent feature of item i.
• Fjk represents the kth latent feature of additional item feature j.
• K denotes the dimensionality of the latent feature space.
"""

# Create the ratings matrix R, from provided ratings
R = np.zeros((num_users, num_items))
for rating, user_idx, item_idx in zip(ratings, user_indices, item_indices):
    R[user_idx, item_idx] = rating

print(R)

def mf_with_features(R, additional_features, k, n_epoch=5000, lr=.0003, l2=.04):
    tol = .001  # Tolerant loss.
    m, n = R.shape
    # Initialize the embedding weights and additional feature weights
    P = np.random.rand(m, k)
    Q = np.random.rand(n, k)
    F = np.random.rand(additional_features.shape[1], k)
    for epoch in range(n_epoch):
        # Update weights by gradients.
        for u, i in zip(*R.nonzero()):
            err_ui = R[u,i] - np.dot(P[u,:] + np.dot(additional_features[u], F), Q[i,:])
            for j in range(k):
                P[u][j] += lr * (2 * err_ui * Q[i][j] - l2/2 * P[u][j])
                Q[i][j] += lr * (2 * err_ui * (P[u][j] + np.dot(additional_features[u], F[:,j])) - l2/2 * Q[i][j])
                F[:,j] += lr * (2 * err_ui * Q[i][j] * additional_features[u] - l2/2 * F[:,j])
        # compute the loss.
        E = (R - np.dot((P + np.dot(additional_features, F)), Q.T))**2
        obj = E[R.nonzero()].sum() + lr*((P**2).sum() +(Q**2).sum())
        if obj < tol:
            break
    return P, Q, F

# Set embedding dimension and train the model
k = 10  # You can adjust this value based on your preference
P, Q, F = mf_with_features(R, additional_features, k)

# Define the maximum rating value in your dataset
max_rating = np.max(R)

# Generating predicted ratings for users in your dataset
predicted_ratings = np.dot((P[user_indices] + np.dot(additional_features, F)), Q.T)

# Post-processing: Clamp predicted ratings to the range [0, max_rating]
predicted_ratings = np.clip(predicted_ratings, 0, max_rating)


predicted_ratings_rounded = np.round(predicted_ratings, decimals=1)
predicted_ratings_masked = np.where(R != 0, 0, predicted_ratings_rounded)

# Making item recommendations for each user
top_n = 5  # Number of top recommendations
recommended_item_ids = []
for ratings in predicted_ratings_masked:
    top_indices = np.argsort(ratings)[::-1][:top_n]
    recommended_item_ids.append(top_indices + 1)  # Convert zero-based indices to original item IDs

print("Recommended Item IDs for each user:")
for user_id, rec_ids in zip(user_ids, recommended_item_ids):
    print(f"User {user_id}: {rec_ids}")

print(predicted_ratings_masked)

# Extract item ID and item name columns from your DataFrame
item_id_name_mapping = {0: 'Blouse', 1: 'Sweater', 2: 'Jeans', 3: 'Sandals', 4: 'Sneakers', 5: 'Shirt', 6: 'Shorts',
                        7: 'Coat', 8: 'Handbag', 9: 'Shoes', 10: 'Dress', 11: 'Skirt', 12: 'Sunglasses', 13: 'Pants',
                        14: 'Jacket', 15: 'Hoodie', 16: 'Jewelry', 17: 'T-shirt', 18: 'Scarf', 19: 'Hat', 20: 'Socks',
                        21: 'Backpack', 22: 'Belt', 23: 'Boots', 24: 'Gloves'}

# fetching the items that were rated within the first 10 users
rated_items = [item_id_name_mapping[item_id] for item_id in item_ids]
rated_items = list(set(rated_items))

# Print the rated items
print(rated_items)

#Items already rated by users

items_rated = pd.DataFrame(R, columns=rated_items[:R.shape[1]], index=user_ids)

items_rated

#Predicted ratings
pred_ratings = pd.DataFrame(predicted_ratings_masked, columns=rated_items[:predicted_ratings_masked.shape[1]], index=user_ids)

pred_ratings

"""##NN IMPLEMENTATION OF MATRIX FACTORIZATION"""

import tensorflow as tf

def create_updated_mf_model(data, user_col='customerid', item_col='itemid', k=10, dropout_rate=0.2, with_bias=False):
    # Preprocessing data
    max_userid = data[user_col].max()
    max_itemid = data[item_col].max()

    # Model architecture
    user_inputs = tf.keras.layers.Input(shape=(1,), name="user")
    item_inputs = tf.keras.layers.Input(shape=(1,), name="item")
    user_embeddings = tf.keras.layers.Embedding(
        input_dim=max_userid + 1, output_dim=k, name="user_embedding")(user_inputs)
    user_embeddings = tf.keras.layers.Dropout(dropout_rate)(user_embeddings)
    item_embeddings = tf.keras.layers.Embedding(
        input_dim=max_itemid + 1, output_dim=k, name="item_embedding")(item_inputs)
    item_embeddings = tf.keras.layers.Dropout(dropout_rate)(item_embeddings)
    dots = tf.keras.layers.Dot(axes=-1, name="logits")([user_embeddings, item_embeddings])

    if with_bias:
        user_biases = tf.keras.layers.Embedding(
            input_dim=max_userid + 1, output_dim=1, name="user_bias")(user_inputs)
        item_biases = tf.keras.layers.Embedding(
            input_dim=max_itemid + 1, output_dim=1, name="item_bias")(item_inputs)
        dots = tf.keras.layers.Add()([dots, user_biases, item_biases])

    model = tf.keras.Model(
        name="updated_matrix_factorizer",
        inputs=[user_inputs, item_inputs],
        outputs=dots
    )

    model.compile(
        optimizer=tf.keras.optimizers.Adam(),
        loss=tf.keras.losses.MeanSquaredError(),
        metrics=[
            tf.keras.metrics.MeanSquaredError(),
            tf.keras.metrics.RootMeanSquaredError(),
            tf.keras.metrics.MeanAbsoluteError()
        ]
    )

    return model

# # Create the updated model
# updated_model = create_updated_mf_model(data, user_col='customerid', item_col='itemid', k=10, dropout_rate=0.2)

# # Print the summary of the updated model
# updated_model.summary()

def fit_model(model, X, y, batch_size=32, epochs=10, validation_split=0.1):
    model.fit(
        x=X,
        y=y,
        batch_size=batch_size,
        epochs=epochs,
        validation_split=validation_split
    )

# Create the matrix factorization model with dropout layers
mf_model = create_updated_mf_model(matrixfact_data, user_col='customerid', item_col='itemid', k=10)
X = [matrixfact_data['customerid'].values, matrixfact_data['itemid'].values]
y = matrixfact_data['ratings'].values
fit_model(mf_model, X, y)

predictions = mf_model.predict([matrixfact_data['customerid'].values, matrixfact_data['itemid'].values])
print(predictions[:5])

mf_model.summary()

# Convert user_ids and item_ids to one-dimensional arrays
user_ids = matrixfact_data['customerid'].values.flatten()
item_ids = matrixfact_data['itemid'].values.flatten()


# Combine user IDs, item IDs, and predictions into a DataFrame
predictions_df = pd.DataFrame({'customerid': user_ids, 'itemid': item_ids, 'predicted_rating': predictions})

# Sort predictions to get top 5 recommended items for each user
top5_recommendations = predictions_df.groupby('customerid').apply(lambda x: x.nlargest(5, 'predicted_rating')).reset_index(drop=True)

# # Print top 5 recommended items for each user
# for user_id, recommendations in top5_recommendations.groupby('customerid'):
#     print(f"Top 5 recommended items for User {user_id}:")
#     for idx, row in recommendations.iterrows():
#         print(f"Item ID: {row['itemid']}, Predicted Rating: {row['predicted_rating']}")
#     print()

top5_recommendations

"""**Model Architecture:**
######The neural network architecture allows for non-linear interactions between user and
item features, capturing more complex patterns in the data compared to traditional
matrix factorization. The neural network model consists of the following components:
25
##• Embedding Layers: Embedding layers are a fundamental component of neural
networks for handling categorical data. Each user and item is represented as a
dense vector in the embedding space.
##• Dot Product or Similarity Layer:
After obtaining the user and item embeddings, they are fed into a dot product
layer. The layer computes the similarity between user and item embeddings,
often using dot product. The output of this layer represents the predicted rating
or score for each user-item pair, indicating the likelihood of a user’s preference
for an item.
##• Loss Calculation: After obtaining the predicted ratings for user-item pairs,
they are compared with the actual ratings present in the training data. The loss
function, such as Mean Squared Error (MSE), is then calculated based on the
discrepancy between the predicted and actual ratings. The loss function quantifies
how well the model is performing in terms of predicting user preferences.
##• Backpropagation: Backpropagation is a fundamental step in training neural
networks. In this step, the gradients of the loss function with respect to the model
parameters (embedding weights) are computed. These gradients indicate how the
loss would change with respect to small changes in the embedding weights.
##• Parameter Updates: The optimizer utilizes the computed gradients to update
the embedding weights of the neural network. The optimizer adjusts the weights
in a way that minimizes the loss function, effectively improving the model’s ability
to make accurate predictions. Common optimization algorithms like stochastic
gradient descent (SGD) or Adam optimizer are used for this purpose.
##• Iterative Training: Steps 2 to 6 are repeated iteratively for multiple epochs.
During each epoch, the model learns from the training data, computes gradients,
updates parameters, and adjusts its predictions. Iterative training continues until
the model converges, meaning the loss stops decreasing significantly or reaches a
predefined threshold.
##• L2 Regularization: L2 regularization, commonly known as weight decay, has
been employed to prevent overfitting and enhance generalization. During training,
a penalty term proportional to the square of the weights’ magnitudes is added to
the loss function. This discourages large weights, promoting simpler models that
generalize better to unseen data. By controlling model complexity, L2 regulariza-
tion effectively reduces overfitting, thus improving performance on validation or
test data.
##• Dropout: Dropout, a regularization technique introduced by Srivastava et al. in
2014, has been integrated into the system. During training iterations, a fraction
of neurons in the neural network are randomly dropped out (set to zero) with a
predefined probability P, typically ranging between 0.2 and 0.5. Dropout prevents
the model from overly depending on specific features or neurons, fostering the
learning of more robust and generalizable representations. By introducing noise
during training, Dropout encourages neurons to learn more independent features,
26
leading to enhanced generalization performance. During inference, Dropout is
disabled, allowing the full network to be utilized for making predictions. Dropout
is particularly effective in mitigating overfitting, especially in deep neural networks
with numerous parameters and layers.

## DATA VISUALIZATION
"""

plt.figure(figsize=(7, 4))

sns.countplot(x='Promo Code Used', data=data, palette='muted')
plt.title('Promo Code Usage')
plt.xlabel('Promo Code Used')
plt.ylabel('Count')

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.countplot(x='Shipping Type', data=data, palette='Set3')
plt.title('Shipping Type Distribution')
plt.xlabel('Shipping Type')
plt.ylabel('Count')

plt.tight_layout()

plt.figure(figsize=(7, 4))

sns.countplot(x='Subscription Status', data=data, palette='Set2')
plt.title('Subscription Status')
plt.xlabel('Subscription Status')
plt.ylabel('Count')

plt.figure(figsize=(7, 4))

# plt.subplot(1, 2, 1)
sns.histplot(data['Review Rating'], bins=10, kde=True, color='orange')
plt.title('Review Ratings Distribution')
plt.xlabel('Review Rating')
plt.ylabel('Frequency')

"""With an average review rating of around 3.75 out of 5, providing
insight to businesses to promptly address any issues and maintain high levels of satis-
faction and loyalty. Standard Deviation is approximately 0.72. minimum Rating is 2.5.
Interquartile Range (IQR) is from 3.1 to 4.4
"""

plt.figure(figsize=(8, 6))
sns.countplot(y='Frequency of Purchases', data=data, palette='Set2')
plt.title('Frequency of Purchases')
plt.xlabel('Count')
plt.ylabel('Frequency of Purchases')
plt.show()

plt.figure(figsize=(6, 6))
data['Gender'].value_counts().plot(kind='pie', autopct='%1.1f%%', colors=['lightblue', 'lightgreen'])
plt.title('Gender Distribution')
plt.ylabel('')
plt.show()

"""In the dataset, 68 percent of the people are male while 32 percent are female as
shown in the figure below. Understanding these gender dynamics can enable businesses
to tailor their marketing messages and product assortments to better resonate with
their target audience (Brown and Lee, 2022).
30

"""

plt.figure(figsize=(10, 6))
sns.countplot(y='Item Purchased', data=data, palette='pastel')
plt.title('Item Purchased Count')
plt.xlabel('Count')
plt.ylabel('Item Purchased')
plt.show()

plt.figure(figsize=(7, 4))
sns.histplot(data['Purchase Amount (USD)'], bins=10, kde=True, color='salmon')
plt.title('Purchase Amount Distribution')
plt.xlabel('Purchase Amount (USD)')
plt.ylabel('Frequency')
plt.show()

"""Further analysis shows that the purchase amounts ranges from $20 to $100 and an
average purchase amount of approximately $59. This shows insight into customers’
price sensitivity and willingness to spend. Moreover, understanding the distribution
of purchase amounts enables businesses to optimize pricing strategies and promotional
offers to maximize revenue and profitability
"""

pd.pivot_table(data, columns='Category', values=['Purchase Amount (USD)','Previous Purchases'], aggfunc='sum')

pd.pivot_table(data, columns='Item Purchased', values=['Purchase Amount (USD)','Previous Purchases'], aggfunc='sum')

plt.figure(figsize=(7, 4))
plt.hist(data['Age'], bins=10, color='skyblue', edgecolor='black')  # Adjust bins as needed
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.title('Histogram of Age Distribution')
plt.grid(True)
plt.show()

"""
The age feature is a fundamental demographic variable, ranging from 18 to 70 years,
with a mean age of approximately 44 years.
Figure 9: Example Image
This age distribution offers valuable insights into the target demographic of the
business, allowing for tailored marketing strategies and product offerings."""

data['Frequency of Purchases'].value_counts()

item_features.head()

# Heatmap to visualize correlations between numerical variables
plt.figure(figsize=(10, 6))
sns.heatmap(item_features.corr(), annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()