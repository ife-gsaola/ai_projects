# -*- coding: utf-8 -*-
"""Workshop_3_Classification Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hmgE4mLup10mSj7vCiyEibTX-wDRn8Kz
"""

# Accessing google drive
from google.colab import drive
drive.mount('/content/drive')

############# WRITE YOUR CODE IN THIS CELL (IF APPLICABLE)####################

# Importing necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt

# Step 1: Data Preprocessing
# Load the dataset
data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/DATA MINING/adult_WS#3.csv")

data.head()

data.info()

data.isna().sum()

data.shape

# Handling missing values by droping affected columns
data.dropna(inplace=True)

data.shape

# Separate features (X) and target variable (y)
X = data.drop('income', axis=1)
y = data['income']

# Encode ordinal and nominal columns using OrdinalEncoder
ordinal_cols = ['sex','workclass','education', 'marital-status', 'occupation', 'relationship', 'race', 'native-country']
ordinal_encoder = OrdinalEncoder()
X[ordinal_cols] = ordinal_encoder.fit_transform(X[ordinal_cols])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize classifiers
svm_classifier = SVC()
dt_classifier = DecisionTreeClassifier(random_state=42)
rf_classifier = RandomForestClassifier(random_state=42)
knn_classifier = KNeighborsClassifier()

# Train classifiers
svm_classifier.fit(X_train, y_train)
dt_classifier.fit(X_train, y_train)
rf_classifier.fit(X_train, y_train)
knn_classifier.fit(X_train, y_train)

#Step 3: prediction
y_pred1=svm_classifier.predict(X_test)
y_pred2=dt_classifier.predict(X_test)
y_pred3=rf_classifier.predict(X_test)
y_pred4=knn_classifier.predict(X_test)

# Creating the confusion matrics for all classifiers' predictions
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm1 = confusion_matrix(y_test, y_pred1, labels=svm_classifier.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm1,display_labels=svm_classifier.classes_)
disp.plot()
plt.title("SVM")

cm2 = confusion_matrix(y_test, y_pred2, labels=rf_classifier.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm2,display_labels=rf_classifier.classes_)
disp.plot()
plt.title("RF")

cm3 = confusion_matrix(y_test, y_pred3, labels=knn_classifier.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm3,display_labels=knn_classifier.classes_)
disp.plot()
plt.title("KNN")

cm4 = confusion_matrix(y_test, y_pred4, labels=dt_classifier.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm4,display_labels=dt_classifier.classes_)
disp.plot()
plt.title("DT")

# This function takes the confusion matrix (cm) from the cell above and produces all evaluation matrix
def confusion_metrics (conf_matrix):

    TP = conf_matrix[1][1]
    TN = conf_matrix[0][0]
    FP = conf_matrix[0][1]
    FN = conf_matrix[1][0]
    print('True Positives:', TP)
    print('True Negatives:', TN)
    print('False Positives:', FP)
    print('False Negatives:', FN)

    # calculate accuracy
    conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))

    # calculate mis-classification
    conf_misclassification = 1- conf_accuracy

    # calculate the sensitivity
    conf_sensitivity = (TP / float(TP + FN))
    # calculate the specificity
    conf_specificity = (TN / float(TN + FP))

    # calculate precision
    conf_precision = (TN / float(TN + FP))
    # calculate f_1 score
    conf_f1 = 2 * ((conf_precision * conf_sensitivity) / (conf_precision + conf_sensitivity))
    print('-'*50)
    print(f'Accuracy: {round(conf_accuracy,2)}')
    print(f'Mis-Classification: {round(conf_misclassification,2)}')
    print(f'Sensitivity: {round(conf_sensitivity,2)}')
    print(f'Specificity: {round(conf_specificity,2)}')
    print(f'Precision: {round(conf_precision,2)}')
    print(f'f_1 Score: {round(conf_f1,2)}')

#printing the evaluation metrics for all classifiers
print('SVM metrics\n')
confusion_metrics(cm1)
print('\n\n')
print('RF metrics\n')
confusion_metrics(cm2)
print('\n\n')
print('KNN metrics\n')
confusion_metrics(cm3)
print('\n\n')
print('DT metrics\n')
confusion_metrics(cm4)
print('\n\n')

# Step 4: Feature Importance
# Random Forest has feature importance attribute, we'll use it
importance = rf_classifier.feature_importances_
plt.figure(figsize=(10, 6))
plt.barh(X.columns, importance)
plt.title("Random Forest Feature Importance")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.show()

# Step 5: Identify Important Features
# Determine the three most important features
important_features = pd.Series(importance, index=X.columns).nlargest(3)
print("\nThree Most Important Features:")
print(important_features)