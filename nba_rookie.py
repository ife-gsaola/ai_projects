# -*- coding: utf-8 -*-
"""Neural Network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i0NRXU-XzL5DHxTQRgyd5rsJTvcUNlmX
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier

import matplotlib.pyplot as plt
import seaborn as sns

sns.set() #Adjust seaborn ploting and adding gridlines

from google.colab import files
uploaded = files.upload()

data = pd.read_csv('nba_rookie_data.csv')
# data.drop('Name', axis=1).corr()

data.describe(include='all')

data2 = data.drop('Name', axis=1)
data2.dropna(inplace=True)
X = data2.drop('TARGET_5Yrs', axis=1)
y = data2['TARGET_5Yrs']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=0)

# Data Preprocessing

X.isnull().sum()

# Checking the distribution of the data for normalcy

# sns.pairplot(data=X, palette="viridis")

cor = data2.corr()
features = pd.DataFrame()
# features['new'] = cor['TARGET_5Yrs'].

features = features.append(cor['TARGET_5Yrs'], ignore_index=True)

features[features>0.1]
# features

ftrs = ['Games Played', 'Minutes Played', 'Points Per Game', 'Field Goals Made',
       'Field Goal Attempts', 'Field Goal Percent', '3 Point Made',
       '3 Point Attempt', '3 Point Percent', 'Free Throw Made',
       'Free Throw Attempts', 'Free Throw Percent', 'Offensive Rebounds',
       'Defensive Rebounds', 'Rebounds', 'Assists', 'Steals', 'Blocks',
       'Turnovers', 'TARGET_5Yrs']

ftrs3 = ftrs = ['Games Played', 'Minutes Played', 'Points Per Game', 'Field Goals Made',
       'Field Goal Attempts', 'Field Goal Percent', 'Free Throw Made',
       'Free Throw Attempts', 'Free Throw Percent', 'Offensive Rebounds',
       'Defensive Rebounds', 'Rebounds', 'Assists', 'Steals', 'Blocks',
       'Turnovers', 'TARGET_5Yrs']

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

models = {
    'LogisticRegression': LogisticRegression(solver='liblinear', max_iter=1000),
    'GaussianNaiveBayes': GaussianNB(),
    'NeuralNetwork': MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000)
}

# Train and evaluate models
for model_name, model in models.items():

    # For Neural Network, use scaled features
    if model_name == 'NeuralNetwork':
        model.fit(X_train_scaled, y_train)
    else:
        model.fit(X_train, y_train)

def evaluate_model(model, X_test_data, y_true, model_name):
    predictions = model.predict(X_test_data)
    accuracy = accuracy_score(y_true, predictions)
    report = classification_report(y_true, predictions, output_dict=True)

    # Extract precision, recall, and f1-score for each class
    class_metrics = report['weighted avg']

    # Create a dictionary to store the metrics
    metrics_dict = {
        'Model': model_name,
        'Accuracy': accuracy,
        'Precision': class_metrics['precision'],
        'Recall': class_metrics['recall'],
        'F1-Score': class_metrics['f1-score']
    }

    return metrics_dict

results_before_tuning = []
for model_name, model in models.items():
    # print(f"Evaluating {model_name} (before hyperparameter tuning):")
    model_metrics = evaluate_model(model, X_test_scaled, y_test, model_name)
    results_before_tuning.append(model_metrics)
    # print("\n")

# Create a DataFrame from the results before tuning
results_before_tuning_df = pd.DataFrame(results_before_tuning)

from sklearn.model_selection import GridSearchCV

def hyperparameter_tuning(model, param_grid, X_train_data, y_train_data):
    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')
    grid_search.fit(X_train_data, y_train_data)

    best_model = grid_search.best_estimator_
    best_params = grid_search.best_params_

    print(f"Best parameters for the model: {best_params}")

    return best_model

param_grids = {
    'LogisticRegression': {'C': [0.001, 0.01, 0.1, 1, 10, 100]},
    'GaussianNaiveBayes': {},  # Naive Bayes has no hyperparameters to tune
    'NeuralNetwork': {'hidden_layer_sizes': [(5,), (10,), (15,)], 'alpha': [0.0001, 0.001, 0.01]}
}

results = []
for model_name, model in models.items():
    # print(f"Performing hyperparameter tuning for {model_name}...")
    param_grid = param_grids[model_name]
    best_model = hyperparameter_tuning(model, param_grid, X_train_scaled, y_train)

    # print(f"Evaluating {model_name} (after hyperparameter tuning):")
    model_metrics = evaluate_model(best_model, X_test_scaled, y_test, model_name)
    results.append(model_metrics)
    # print("\n")

# Create a DataFrame from the results
results_df = pd.DataFrame(results)

# print(results_df)

results_before_tuning_df

results_df

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming results_df has columns 'Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', etc.

# Set the style of seaborn
sns.set(style="whitegrid")

# Create subplots for each metric
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))
fig.suptitle('Performance Metrics for Different Models')

# Plot accuracy
sns.barplot(x='Model', y='Accuracy', data=results_df, ax=axes[0, 0])
axes[0, 0].set_title('Accuracy')

# Plot precision
sns.barplot(x='Model', y='Precision', data=results_df, ax=axes[0, 1])
axes[0, 1].set_title('Precision')

# Plot recall
sns.barplot(x='Model', y='Recall', data=results_df, ax=axes[1, 0])
axes[1, 0].set_title('Recall')

# Plot F1 Score
sns.barplot(x='Model', y='F1-Score', data=results_df, ax=axes[1, 1])
axes[1, 1].set_title('F1 Score')

# Adjust layout
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()