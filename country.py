# -*- coding: utf-8 -*-
"""Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m_V7feS8VpIKFjWAtLwXas5WXdvw2zjM
"""

#importing necessary libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

sns.set() #Adjust seaborn ploting and adding gridlines

#importing dataset

from google.colab import files
uploaded = files.upload()

#loading dataset into dataframe

data = pd.read_csv('country_data.csv')
data.head()

# data.isnull().count()

#Make a copy of the dataset not to tamper with the original data.
data2 = data.copy()

data.corr()

#taking a look at the dataset to see the percentiles for how the data is distributed
data2.describe(include='all')

#removing suspicious outliers that were noticed in the data. However, investigation is required before removing outlier in real life.
q = data2['child_mort'].quantile(0.95)
data2 = data2[data2['child_mort']<q]

q2 = data2['exports'].quantile(0.95)
data2 = data2[data2['exports']<q2]

q3 = data2['income'].quantile(0.95)
data2 = data2[data2['income']<q3]

#
data2.shape , data.shape

data2.describe(include='all')

Xcolumns = ['child_mort', 'exports', 'health', 'imports', 'income',
       'inflation', 'life_expec', 'total_fer', 'gdpp']
num_cols = len(Xcolumns)
num_rows = (num_cols - 1) // 3 + 1  # Calculate the number of rows needed

f, axs = plt.subplots(num_rows, 3, figsize=(10, 6))
for i in range(num_cols):
    row_index = i // 3
    col_index = i % 3
    sns.scatterplot(data2[Xcolumns[i]], ax=axs[row_index, col_index])

# Remove any extra empty subplots
for i in range(num_cols, num_rows * 3):
    f.delaxes(axs.flatten()[i])

plt.tight_layout()  # Automatically adjust subplot parameters to avoid overlapping
plt.show()

from sklearn import preprocessing
x_scaled = preprocessing.scale(data2.drop('country', axis=1))
x_scaled

from sklearn.cluster import KMeans

SSE = []
for cluster in range(1,10):
    kmeans = KMeans(n_clusters=cluster, init='k-means++', random_state=42)
    kmeans.fit(x_scaled)
    SSE.append(kmeans.inertia_)

plt.plot(range(1,10), SSE)
plt.xlabel('Number of clusters')
plt.ylabel('SSE')

kmeans_ = KMeans(5)
kmeans_.fit(x_scaled)

cluster_ = data2.copy()
cluster_['cluster_pred'] = kmeans_.fit_predict(x_scaled)

unique_clusters = cluster_['cluster_pred'].unique()

plt.scatter(cluster_['child_mort'], cluster_['life_expec'], c=cluster_['cluster_pred'], cmap='rainbow')
plt.xlabel('Child Mortality')
plt.ylabel('Life Expectancy')

# Add cluster labels
for cluster_label in unique_clusters:
    # Get the centroid or representative point for the cluster
    centroid = cluster_.loc[cluster_['cluster_pred'] == cluster_label, ['child_mort', 'life_expec']].mean()

    # Annotate the centroid with the cluster label
    plt.annotate(f'{cluster_label}', (centroid['child_mort'], centroid['life_expec']), ha='center', va='center',
                 bbox=dict(boxstyle='round,pad=0.3', edgecolor='black', facecolor='white'))

plt.show()

X_scaled = data2[['child_mort','exports',	'imports','income', 'inflation', 'gdpp']]

from sklearn import preprocessing
x_scaled = preprocessing.scale(data2.drop('country', axis=1))
x_scaled

SSE = []
for cluster in range(1,10):
    kmeans = KMeans(n_clusters=cluster, init='k-means++', random_state=42)
    kmeans.fit(x_scaled)
    SSE.append(kmeans.inertia_)

plt.plot(range(1,10), SSE)
plt.xlabel('Number of clusters')
plt.ylabel('SSE')

kmeans_ = KMeans(4)
kmeans_.fit(x_scaled)

cluster_ = data2.copy()
cluster_['cluster_pred'] = kmeans_.fit_predict(x_scaled)

unique_clusters = cluster_['cluster_pred'].unique()

plt.scatter(cluster_['child_mort'], cluster_['inflation'], c=cluster_['cluster_pred'], cmap='rainbow')
plt.xlabel('Child Mortality')
plt.ylabel('inflation')

# Add cluster labels
for cluster_label in unique_clusters:
    # Get the centroid or representative point for the cluster
    centroid = cluster_.loc[cluster_['cluster_pred'] == cluster_label, ['child_mort', 'inflation']].mean()

    # Annotate the centroid with the cluster label
    plt.annotate(f'{cluster_label}', (centroid['child_mort'], centroid['inflation']), ha='center', va='center',
                 bbox=dict(boxstyle='round,pad=0.3', edgecolor='black', facecolor='white'))

plt.show()

from sklearn.decomposition import PCA

# Apply PCA for dimensionality reduction
pca = PCA(n_components=2)
data_pca = pca.fit_transform(x_scaled)

SSE = []
for cluster in range(1,10):
    kmeans = KMeans(cluster)
    kmeans.fit(data_pca)
    SSE.append(kmeans.inertia_)

plt.plot(range(1,10), SSE)
plt.xlabel('Number of clusters')
plt.ylabel('SSE')

kmeans_ = KMeans(4)

yy = kmeans_.fit_predict(data_pca)
cluster_ = data2.copy()
# cluster_['cluster_pred'] = kmeans_.fit_predict(data_pca)

plt.scatter(data_pca[:, 0], data_pca[:, 1], c=kmeans.labels_, cmap='viridis')
plt.title('KMeans Clustering - Original Features')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()
