# -*- coding: utf-8 -*-
"""Workshop_4_Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ra3eyyRfxng5AdUUhtoYgI_uIYwoCySk
"""

# Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.preprocessing import StandardScaler

# Accessing google drive
from google.colab import drive
drive.mount('/content/drive')

# Step 1: Data Preprocessing
# Load the dataset
data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/DATA MINING/adult_WS4.csv")

data.shape

data.head()

data.replace('?', np.nan, inplace=True)

data.dropna(inplace=True)

data.shape

data.head(2)

# Selecting three optional columns
X = data.select_dtypes(include=['number'])
y=data['education']

encoder = LabelEncoder()
y = encoder.fit_transform(y)

# Step 2: Preprocess the data (if necessary)
# Check for missing values
missing_values = X.isnull().sum()
print("Missing Values:")
print(missing_values)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# K-Means Clustering
# Using the Elbow Method to find the optimal number of clusters
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

# Plot the Elbow Method
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method for K-Means Clustering')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

# Hierarchical Clustering
# Using Dendrogram to find the optimal number of clusters
linked = linkage(X_scaled, method='ward')
plt.figure(figsize=(10, 5))
dendrogram(linked)
plt.title('Dendrogram for Hierarchical Clustering')
plt.xlabel('Samples')
plt.ylabel('Distance')
plt.show()

kmeans = KMeans(n_clusters=4, init= 'k-means++', random_state=8)

predicted = kmeans.fit_predict(X_scaled)

predicted

"""


# The code initially loads the 'adult_WS4' dataset, preprocesses it by handling missing values, selects three optional columns, and encodes the categorical target variable. It then applies StandardScaler to normalize the dataset before conducting K-Means clustering and Hierarchical clustering. The optimal number of clusters for each method is determined using the Elbow Method and Dendrogram, respectively. Finally, K-Means clustering is performed with the identified optimal number of clusters, and the predicted cluster labels are obtained.


#Word count: 76


"""


# Importing necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder

# Step 5: Apply PCA to extract the first two principal components
pca = PCA(n_components=3)
principal_components = pca.fit_transform(X_scaled)

#PCA
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
X_df = pd.DataFrame(data = X_pca, columns = ['principal component 1', 'principal component 2'])

# Step 6: Plot the scatter plot of the dataset's first two components for the two classes of the "income" column
plt.figure(figsize=(10, 6))
plt.scatter(principal_components[:, 0], principal_components[:, 1], c=y, cmap='viridis')
plt.title('PCA: Scatter Plot of First Two Principal Components')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(label='Income Class')
plt.show()

"""


#The code implements Principal Component Analysis (PCA) to extract the first two principal components from the normalized dataset. It then plots a scatter plot of these principal components, coloring the points based on the income classes. This visualization provides insights into the distribution and separation of the data points in the reduced-dimensional space. Overall, the code effectively applies clustering and dimensionality reduction techniques for exploratory analysis of the dataset.


#Word count: 75


"""