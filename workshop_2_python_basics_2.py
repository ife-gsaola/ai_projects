# -*- coding: utf-8 -*-
"""Workshop_2_Python Basics 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E9i95hKRQu4O5hUdpfE0Dl2lqe9cmBq1
"""

# Accessing google drive
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/DATA MINING/adult (2).csv")


# Assuming `data` is your DataFrame
null_values_per_column = data.isna().sum()
columns_with_null_values = null_values_per_column[null_values_per_column > 0]

print("a) Columns with Null values:")
print(columns_with_null_values)

# Change display settings to show all columns and rows
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

# Print the first 100 rows of the DataFrame
data.head(100)

"""

#data.head(100): This line prints the first 100 rows of the DataFrame data after the display settings have been adjusted. The head(100) method returns the first 100 rows of the DataFrame, and print() displays it in the output.


##C.

1. Data Profiling: Generate summary statistics for each column, such as mean, median, mode, standard deviation, minimum, and maximum values. Look for any outliers or unexpected values that deviate significantly from the expected range or distribution.

2. Data Quality Checks: Implement automated data quality checks to validate the integrity, consistency, and accuracy of the data. For example, check for missing values, duplicates, or data format errors in each column.

3. Domain Knowledge: Leverage domain knowledge or subject matter expertise to define rules or constraints that the data should adhere to. Use these rules to validate the data and identify any columns that violate these constraints.

4. Data Validation Rules: Define validation rules or constraints specific to each column based on its data type and semantics. For example, ensure that numeric columns contain only numeric values, categorical columns contain predefined categories, and date columns follow a specific format.

5. Statistical Analysis: Perform statistical analysis to identify anomalies or inconsistencies in the data distribution. Use techniques such as hypothesis testing, correlation analysis, or clustering to detect patterns or abnormalities in the data.

6. Machine Learning Models: Train machine learning models to detect anomalies or outliers in the data. Use unsupervised learning techniques like clustering or anomaly detection algorithms to identify columns with unusual or unexpected values.
"""


# Get the data types of columns containing null values
columns_with_null = data.columns[data.isna().any()]
data_types = data[columns_with_null].dtypes

# Report the data types
print("Data types of columns with null values:")
for column, data_type in data_types.items():
    print(f"{column}: {data_type}")

# Delete rows with null values
data_cleaned = data.dropna()

# Print the number of null values in each column
print("Number of Null values of all columns after removing rows with Null values:")
print(data_cleaned.isna().sum())

data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/DATA MINING/adult (2).csv")

# Find categorical columns with null values
categorical_columns_with_null = data.select_dtypes(include=['object']).columns[data.select_dtypes(include=['object']).isna().any()]

# Fill null values with mode for each categorical column
for column in categorical_columns_with_null:
    mode_value = data[column].mode()[0]
    data[column].fillna(mode_value, inplace=True)

# Print the number of null values in each column after filling null values
print("Number of Null values of all columns after filling null values with mode:")
print(data.isna().sum())



# Assuming 'age' is the column with wrong data
# Calculate the median age
median_age = data['age'].median()

# Define a condition to identify wrong values (e.g., age < 0 or age > 100)
wrong_age_condition = (data['age'] < 0) | (data['age'] > 100)

# Fill wrong cells with the median age
data.loc[wrong_age_condition, 'age'] = median_age

# Verify that the wrong cells have been filled with the median age
print("Number of wrong values filled:", wrong_age_condition.sum())

data.head()

# Using LabelEncoder for binary classification
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
data['income_encoded'] = label_encoder.fit_transform(data['income'])

# Using LabelEncoder for ordinal encoding
data['education-num_encoded'] = label_encoder.fit_transform(data['education-num'])

# Using One-Hot Encoding for nominal encoding
data = pd.get_dummies(data, columns=['workclass'], prefix='workclass')

# Numerical columns are ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
numerical_columns = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])